{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=30522,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.1,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        pad_token_id=0,\n",
    "        gradient_checkpointing=False,\n",
    "        position_embedding_type=\"absolute\",\n",
    "        use_cache=True\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        self.position_embedding_type = position_embedding_type\n",
    "        self.use_cache = use_cache\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.vocab_size = config.vocab_size # 30522\n",
    "        self.type_vocab_size = config.type_vocab_size # 2\n",
    "        self.hidden_size = config.hidden_size # 768\n",
    "        self.max_position_embeddings = config.max_position_embeddings # 512\n",
    "        self.embeddings_sum = tf.keras.layers.Add()\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        self.weight = self.add_weight(shape=[self.vocab_size, self.hidden_size])\n",
    "        self.token_type_embedding = self.add_weight(shape=[self.type_vocab_size, self.hidden_size])\n",
    "        self.position_embedding = self.add_weight(shape=[self.max_position_embeddings, self.hidden_size])\n",
    "        \n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        position_ids=None,\n",
    "        token_type_ids=None,\n",
    "        training=False\n",
    "    ):\n",
    "        inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n",
    "        position_embeds = tf.gather(params=self.position_embedding, indices=position_ids)\n",
    "        token_type_embeds = tf.gather(params=self.token_type_embedding, indices=token_type_ids)\n",
    "        final_embedding = self.embeddings_sum(inputs=[inputs_embeds, position_embeds, token_type_embeds])\n",
    "        final_embedding = self.LayerNorm(inputs=final_embedding)\n",
    "        final_embedding = self.dropout(inputs=final_embedding, training=training)\n",
    "        # shape = [batch_size, seq_len, hidden_size]\n",
    "        return final_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads # 12\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads) # 768/12\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size # 768\n",
    "        self.sqrt_att_head_size = math.sqrt(self.attention_head_size) # sqrt(768)\n",
    "        self.query = tf.keras.layers.Dense(units=self.all_head_size)\n",
    "        self.key = tf.keras.layers.Dense(units=self.all_head_size)\n",
    "        self.value = tf.keras.layers.Dense(units=self.all_head_size)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=config.attention_probs_dropout_prob)\n",
    "        \n",
    "    def transpose_for_scores(self, tensor, batch_size):\n",
    "        tensor = tf.reshape(tensor=tensor, shape=[batch_size, -1, self.num_attention_heads, self.attention_head_size])\n",
    "        return tf.transpose(tensor, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask,\n",
    "        head_mask,\n",
    "        output_attentions,\n",
    "        traning=False\n",
    "    ):\n",
    "        batch_size = hidden_states.shape.as_list()[0]\n",
    "        # mixed layer includes all the heads\n",
    "        # shape = [batch_size, seq_len, all_head_size]\n",
    "        mixed_query_layer = self.query(inputs=hidden_states)\n",
    "        mixed_key_layer = self.key(inputs=hidden_states)\n",
    "        mixed_value_layer = self.value(inputs=hidden_states)        \n",
    "        # shape = [batch_size, num_attention_heads=12, seq_len=512, attention_head_size=768/12]\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n",
    "        # shape = [batch_size, num_attention_heads, seq_len, seq_len]\n",
    "        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
    "        dk = tf.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)\n",
    "        attention_scores = tf.divide(attention_scores, dk)\n",
    "        attention_scores = tf.add(attention_scores, attention_mask)\n",
    "        attention_probs = tf.nn.softmax(logits=attention_scores, axis=-1)\n",
    "        attention_probs = tf.multiply(attention_probs, head_mask)\n",
    "        # shape = [batch_size, num_attention_heads, seq_len, attention_head_size]\n",
    "        attention_output = tf.matmul(attention_probs, value_layer)\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        # shape = [batch_size, seq_len, all_head_size]\n",
    "        outputs = tf.reshape(attention_output, shape=[batch_size, -1, self.all_head_size])\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1, 768), dtype=float32, numpy=\n",
       "array([[[-1.0565512 , -1.1522021 , -1.804117  , ..., -0.21669677,\n",
       "          1.4941266 , -1.4558128 ]],\n",
       "\n",
       "       [[-1.0245496 , -1.4149065 , -1.8073409 , ..., -0.08891329,\n",
       "          1.6286561 , -1.2214249 ]],\n",
       "\n",
       "       [[-0.70252836, -1.2980103 , -2.0195045 , ..., -0.19623536,\n",
       "          1.535229  , -1.154598  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig()\n",
    "h = TFBertEmbedding(config)([1,2,3],[0,0,0],[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([12,3], dtype=tf.int32)\n",
    "a.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 2), dtype=int32, numpy=\n",
       "array([[[3, 3],\n",
       "        [3, 3]]])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.fill(dims=(1,2,3), value=1)\n",
    "b = tf.fill(dims=[1,2,3], value=1)\n",
    "tf.matmul(a, b, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
